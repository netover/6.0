# =============================================================================
# RESYNC v6.2.0 â€” Environment Configuration
# =============================================================================
# Copy to .env and edit:  cp .env.example .env && chmod 600 .env
# All vars can use APP_ prefix (e.g., APP_ENVIRONMENT=production)
# =============================================================================

# -- Application --------------------------------------------------------------
ENVIRONMENT=development          # development | staging | production
APP_PROJECT_NAME=Resync
APP_PROJECT_VERSION=6.2.0
APP_LOG_FORMAT=json              # text | json

# -- Server -------------------------------------------------------------------
APP_HOST=0.0.0.0
APP_PORT=8000
APP_WORKERS=1                    # Uvicorn workers (production: 2-4)

# -- Database (PostgreSQL + pgvector) -----------------------------------------
# Required. Install pgvector extension: CREATE EXTENSION IF NOT EXISTS vector;
DATABASE_URL=postgresql+asyncpg://resync:resync_password@localhost:5432/resync

# Pool settings
APP_DB_POOL_MIN_SIZE=2
APP_DB_POOL_MAX_SIZE=10

# -- Redis --------------------------------------------------------------------
# Optional. Set RESYNC_DISABLE_REDIS=true to run without Redis.
REDIS_URL=redis://localhost:6379/0
RESYNC_DISABLE_REDIS=false

# -- Security & Auth ----------------------------------------------------------
# !! CHANGE THESE IN PRODUCTION - these are development defaults only
SECRET_KEY=dev-secret-key-change-me-in-production-minimum-32-chars
ADMIN_PASSWORD=admin12345678

# -- LLM Configuration -------------------------------------------------------
# LiteLLM-compatible. Supports: openai, anthropic, ollama, azure, etc.
# See: https://docs.litellm.ai/docs/providers
APP_LLM_MODEL=gpt-4o-mini
LLM_API_KEY=sk-your-key-here

# For local models (Ollama):
# APP_LLM_MODEL=ollama/llama3
# APP_LLM_ENDPOINT=http://localhost:11434

# -- Embeddings --------------------------------------------------------------
EMBED_MODEL=text-embedding-3-small
EMBED_DIM=1536

# -- RAG Configuration --------------------------------------------------------
RAG_COLLECTION_WRITE=knowledge_v1
RAG_COLLECTION_READ=knowledge_v1
RAG_MAX_TOPK=50
RAG_HNSW_M=16
RAG_HNSW_EF_CONSTRUCTION=256

# Cross-encoder reranking
# APP_ENABLE_CROSS_ENCODER=true
# RAG_CROSS_ENCODER_MODEL=BAAI/bge-reranker-small

# -- Knowledge Graph ----------------------------------------------------------
APP_KG_EXTRACTION_ENABLED=false

# -- GraphRAG -----------------------------------------------------------------
APP_GRAPHRAG_ENABLED=false

# -- Langfuse (Observability - optional) --------------------------------------
# LANGFUSE_PUBLIC_KEY=pk-...
# LANGFUSE_SECRET_KEY=sk-...
# LANGFUSE_HOST=https://cloud.langfuse.com

# -- Teams Integration (optional) ---------------------------------------------
# TEAMS_OUTGOING_WEBHOOK_ENABLED=false
# TEAMS_OUTGOING_WEBHOOK_SECURITY_TOKEN=
# TEAMS_CALLBACK_URL=

# -- Docling Document Converter ------------------------------------------------
# DOCLING_TABLE_STRUCTURE=true    # ML table extraction (default: true)
# DOCLING_OCR=false               # OCR for scanned documents (default: false)
# DOCLING_PROCESS_TIMEOUT=300     # Subprocess timeout in seconds (default: 300)

# -- Startup policy (ASGI lifespan) -------------------------------------------
# These flags control the canonical startup checks executed in the FastAPI
# lifespan (before the app starts serving traffic).
#
# STARTUP_STRICT:
#   - If set, overrides the default strictness.
#   - If omitted, defaults to "true" in production.
# STARTUP_STRICT=true
#
# Total time budget (seconds) for startup checks (TWS/LLM retries are bounded
# by this budget).
# STARTUP_MAX_TOTAL_SECONDS=30
#
# Generic retry knobs for external services (TWS/LLM).
# STARTUP_SERVICE_RETRIES=3
# STARTUP_RETRY_BASE_DELAY_SECONDS=0.2
# STARTUP_RETRY_MAX_DELAY_SECONDS=2.0
